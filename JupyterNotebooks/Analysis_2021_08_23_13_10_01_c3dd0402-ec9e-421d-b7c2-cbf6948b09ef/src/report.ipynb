{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e130da",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "In the statistics module we analyze data for different responses and at different spectral peak locations.\n",
    "We use Python package scipy in this module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94a828",
   "metadata": {},
   "source": [
    "## T-Test\n",
    "T-test checks for difference in the mean between two sample from different responses. We assume the data is independent and follows the normality assumption.\n",
    "Let $x_1, \\ldots, x_n$ and $y_1,\\ldots ,y_m$ be the two samples and we test whether the means are equal. The null hypothesis states means $\\mu_1$ and $\\mu_2$ are equal\n",
    "and the alternative hypothesis states they are not equal. If the p-value is lower than the chosen significance level, we can reject the null hypothesis, i.e. the samples do not have the same means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee703537",
   "metadata": {},
   "outputs": [],
   "source": [
    "        import modules.adapml_data as adapml_data\n",
    "        import modules.adapml_classification as adapml_classification\n",
    "        import modules.adapml_clustering as adapml_clustering\n",
    "        import modules.adapml_chemometrics as adapml_chemometrics\n",
    "        import modules.adapml_statistics as adapml_statistics\n",
    "        import modules.adapml_regression as adapml_regression\n",
    "        import numpy as np\n",
    "        import modules.loadTestData as load_data\n",
    "        import sklearn.preprocessing as pre\n",
    "        from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "        from matplotlib import pyplot as plt\n",
    "        from sklearn import cluster as clst\n",
    "        from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "        import os\n",
    "\n",
    "        reldir = os.getcwd()\n",
    "        path_to_data = os.path.join(reldir, '..', 'data', 'SCLC_study_output_filtered_2.csv')\n",
    "\n",
    "        data = adapml_data.DataImport(path_to_data)\n",
    "\n",
    "        response1D = data.resp\n",
    "        #response1D = adapml_data.DataImport.getResponse(path_to_data)\n",
    "        response2D = adapml_data.DataImport.getDummyResponse(response1D)\n",
    "\n",
    "        variables = data.getVariableNames()\n",
    "        samples = data.getSampleNames()\n",
    "\n",
    "        t_test = adapml_statistics.Statistics(data.data, 'anova', response1D)\n",
    "        t_test.plot_logp_values(variables)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33a640",
   "metadata": {},
   "source": [
    "## Volcano Plot\n",
    "\n",
    "Volcano plot is a scatter plot which demonstrates magnitude between the responses and t-test significance of the data. We can choose a significance level and fold change limit\n",
    "to specify the rectangle of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test.plot_volcano_t(variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64d8bd",
   "metadata": {},
   "source": [
    "# Dimension-Reduction\n",
    "\n",
    "Dimension-reduction methods are used to condense high dimensional data down to dimensions which provide the most information. We have implemented the principal component analysis (PCA). It performs a change of basis and the new basis is chosen, such that the i-th principal component is orthogonal to the first i-1 principal components and the direction maximizes the variance of the projected data.\n",
    "We use the Python library sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8fc88",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "The principal component analysis (PCA) is one of the methods for dimension-reduction. It performs a change of basis and the new basis is chosen, such that the i-th principal component is orthogonal to the first i-1 principal components and the direction maximizes the variance of the projected data. Instead of considering all the dimensions,\n",
    "we pick the necessary number of principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dde9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.normalizeData(\"autoscale\")\n",
    "\n",
    "pca = adapml_chemometrics.Chemometrics(data.data, \"pca\", response1D)\n",
    "\n",
    "print(\"PCA Projections\");pca.plotProjectionScatterMultiClass(2, labels=[\"Healthy\", \"Not Healthy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98838f66",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Linear discriminant analysis is a classifier with a linear decision boundary. We assume normality and fit conditional densities $p(x\\; | \\; y=0)$ and $p(x \\; | \\; y=1)$ with mean and covariance parameters $(\\mu_0,\\sigma_0)$ and $(\\mu_1,\\sigma_1)$, where $x,\\mu_0$ and $\\mu_1$ are vectors.\n",
    "Dimensionality-reduction is done by projecting the input to the most discriminative directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = adapml_chemometrics.Chemometrics(data.data, \"lda\", response1D) # Also Predicts\n",
    "\n",
    "print(\"LDA Projections\");lda.plotProjectionScatterMultiClass(1, labels=[\"Healthy\", \"Not Healthy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98c6eb",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this module we use various different clustering methods on spectra. We use the elbow method to find the optimal number of clusters. Clustering is done with scipy and sklearn libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039638ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = adapml_clustering.Clustering(data.data, 'silhouette', 3)\n",
    "nr_clusters = silhouette.clustnr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30482506",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-means clustering aims to partition the data into $k$ sets and to minimize the Euclidian within-cluster sum of squares (WCSS). It is solved by either Lloyd’s or Elkan’s algorithm and we use sklearn module in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cluster = adapml_clustering.Clustering(data.data, 'kmeans', nr_clusters)\n",
    "kmeans_cluster.getClusterResults(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79892341",
   "metadata": {},
   "source": [
    "## BIRCH Clustering\n",
    "\n",
    "BIRCH (balance iterative reducing and clustering using hierarchies) is a hierarchical clustering method. The hierarchy is created based on the linear sum and the square sum of data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_cluster = adapml_clustering.Clustering(data.data, 'birch', nr_clusters)\n",
    "birch_cluster.getClusterResults(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713ca88",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering\n",
    "\n",
    "DBSCAN is a non-parametric density-based clustering algorithm. It clusters together nearby neighbors, marking further away points as outliers, as they are in the low density area.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_cluster = adapml_clustering.Clustering(data.data, 'dbscan', nr_clusters)\n",
    "dbscan_cluster.getClusterResults(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe03f5",
   "metadata": {},
   "source": [
    "## Mean Shift Clustering\n",
    "\n",
    "The mean shift algorithm is a nonparametric clustering technique which does not require prior knowledge of the number of clusters, and does not constrain the shape of the clusters. It works by starting at data points and iteratevely finding the convergence points for kernel estimate gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dee4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanshift_cluster = adapml_clustering.Clustering(data.data, 'meanshift', nr_clusters)\n",
    "meanshift_cluster.getClusterResults(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07429cd",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Clustering\n",
    "\n",
    "Gaussian mixture models (GMMs) cluster the data by fitting a mixture of Gaussian models to the data and clustering together data points with similar parameter estimates. It's closely related to k-means clustering but allows for less restrictive cluster shapes. K-means fits a multi-dimensional ball as the perimeter, but GMMs can also fit ellipsoidal shapes and other shapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c429486",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_cluster = adapml_clustering.Clustering(data.data, 'gaussian', nr_clusters)\n",
    "gaussian_cluster.getClusterResults(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626701b",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds hierarchies of clusters based on a chosen metric and a linkage scheme.\n",
    "We used cosine distance and average linkage scheme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_cluster = adapml_clustering.Clustering(data.data, 'hierarchical', nr_clusters)\n",
    "hierarchical_cluster.getClusterResults(samples)\n",
    "hierarchical_cluster.plot_dendrogram(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4786b",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification methods aim to classify the response of samples. The given data is separated into\n",
    "a training set and a testing set. The model parameters are found from the training set and the testing set is used to quantify the model accuracy.\n",
    "The methods are from sklearn package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b239552",
   "metadata": {},
   "source": [
    "## Partial Least Squares-Discriminant Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f54a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProjectionScatterMultiClass(pc, resp, num_var):\n",
    "    plt.figure(figsize=(24, 18))\n",
    "\n",
    "    for i in range(num_var):\n",
    "        for j in range(num_var):\n",
    "            plt.subplot(5,5,5*(i) + j + 1)\n",
    "            for c in range(resp.shape[1]):\n",
    "                inx = np.where(resp[:,c] == 1)[0]\n",
    "                tmp = pc[inx,:]\n",
    "                pc1 = tmp[:,i]\n",
    "                pc2 = tmp[:,j]\n",
    "                plt.scatter(pc1, pc2)\n",
    "            plt.xlabel(\"PLS Component \"+str(i+1))\n",
    "            plt.ylabel(\"PLS Component \"+str(j+1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "data = load_data.loadDataPandas(path_to_data)\n",
    "d = data.to_numpy()\n",
    "var_index = data.columns.values.tolist()\n",
    "\n",
    "resp = load_data.getResponseMatrix2D()\n",
    "\n",
    "norm_trans = pre.StandardScaler().fit(d)\n",
    "data_norm = norm_trans.transform(d)\n",
    "#data_norm, norm_trans = pre.mean_center(d)\n",
    "#In-built preprocessing method - TBD\n",
    "\n",
    "pls = PLS().fit(data_norm, resp)\n",
    "pls_trans = pls.transform(data_norm)\n",
    "\n",
    "plotProjectionScatterMultiClass(pls_trans, resp, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a343cb",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Classification via SVM is done by fitting a linear plane to the latent space but only considering a subset of inputs in the fitting process.\n",
    "The quantity $R^2$ measures what percentage of variation was explained by the model in the training set. The quantity $Q^2$ shows the same measurement but for the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adapml_data.DataImport(path_to_data)\n",
    "svm = adapml_classification.Classification(data.data, response1D, 'svm', .75, kfolds=3)\n",
    "\n",
    "adapml_classification.print_model_stats(svm, \"SVM\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552f2be",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forests is an ensemble classification method.\n",
    "It works by constructing multiple decision trees based on the training data and then choosing the class, chosen by the most number of decision trees.\n",
    "The quantity $R^2$ measures what percentage of variation was explained by the model in the training set. The quantity $Q^2$ shows the same measurement but for the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adapml_data.DataImport(path_to_data)\n",
    "rnf = adapml_classification.Classification(data.data, response1D, 'randomforest', .75, kfolds=3)\n",
    "\n",
    "adapml_classification.print_model_stats(rnf, \"RF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0887dd",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression uses a logistic function to model a binary dependent variable. The confusion matrix displays the accuracy of the model for the test data set.\n",
    "We use the packages sklearn for the logistic regression and seaborn for the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bdd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adapml_data.DataImport(path_to_data)\n",
    "\n",
    "logistic = adapml_classification.Classification(data.data, response1D, 'logistic', .25)\n",
    "print(logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92128d66",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78d4fa",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression fits a linear plane between the dependant variables and the response. The linear plane models the relationship between them and allows for prediction or explain variation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = adapml_regression.Regression(data.data, \"linear\")\n",
    "reg.linear\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
